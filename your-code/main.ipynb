{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Import-and-Describe-the-Dataset\" data-toc-modified-id=\"Challenge-1---Import-and-Describe-the-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Import and Describe the Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?\" data-toc-modified-id=\"Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the dataset with mathematical and visualization techniques. What do you find?</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Data-Cleaning-and-Transformation\" data-toc-modified-id=\"Challenge-2---Data-Cleaning-and-Transformation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Data Cleaning and Transformation</a></span></li><li><span><a href=\"#Challenge-3---Data-Preprocessing\" data-toc-modified-id=\"Challenge-3---Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Data Preprocessing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.\" data-toc-modified-id=\"We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>We will use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> and scale our data. Read more about <code>StandardScaler</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" target=\"_blank\">here</a>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Data-Clustering-with-K-Means\" data-toc-modified-id=\"Challenge-4---Data-Clustering-with-K-Means-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Data Clustering with K-Means</a></span></li><li><span><a href=\"#Challenge-5---Data-Clustering-with-DBSCAN\" data-toc-modified-id=\"Challenge-5---Data-Clustering-with-DBSCAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Data Clustering with DBSCAN</a></span></li><li><span><a href=\"#Challenge-6---Compare-K-Means-with-DBSCAN\" data-toc-modified-id=\"Challenge-6---Compare-K-Means-with-DBSCAN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Compare K-Means with DBSCAN</a></span></li><li><span><a href=\"#Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters\" data-toc-modified-id=\"Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge 2 - Changing K-Means Number of Clusters</a></span></li><li><span><a href=\"#Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples\" data-toc-modified-id=\"Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bonus Challenge 3 - Changing DBSCAN <code>eps</code> and <code>min_samples</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import my libraries\n",
    "\n",
    "# this makes plots show up in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# matplotlib for making plots and charts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy for working with arrays and numbers\n",
    "import numpy as np\n",
    "\n",
    "# pandas for working with data tables (dataframes)\n",
    "import pandas as pd\n",
    "\n",
    "# seaborn makes prettier plots than matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# these next lines just hide some annoying warning messages\n",
    "import warnings                                              \n",
    "from sklearn.exceptions import DataConversionWarning          \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The origin of the dataset is [here](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data: Wholesale customers data\n",
    "customers = pd.read_csv('../data/Wholesale customers data.csv')\n",
    "\n",
    "# let me see what the data looks like\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "* Any categorical data to convert?\n",
    "* Any missing data to remove?\n",
    "* Column collinearity - any high correlations?\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let me explore the data\n",
    "\n",
    "# check how many rows and columns\n",
    "print(\"Shape:\", customers.shape)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# get info about the columns and data types\n",
    "print(\"Info:\")\n",
    "customers.info()\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# check if there are any missing values\n",
    "print(\"Missing values:\")\n",
    "print(customers.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# get basic statistics for each column\n",
    "print(\"Statistics:\")\n",
    "customers.describe()\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# check how columns are related to each other\n",
    "# values close to 1 or -1 mean strong correlation\n",
    "print(\"Correlations:\")\n",
    "correlation_matrix = customers.corr()\n",
    "print(correlation_matrix)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# make a heatmap to visualize correlations\n",
    "# darker colors mean stronger relationships\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# make histograms to see how data is distributed\n",
    "customers.hist(figsize=(15, 10), bins=30)\n",
    "plt.suptitle('Distribution of Each Feature')\n",
    "plt.show()\n",
    "\n",
    "# box plots help me spot outliers (the dots outside the boxes)\n",
    "plt.figure(figsize=(15, 6))\n",
    "customers.boxplot()\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Box Plots - Looking for Outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My observations after exploring the data:**\n",
    "\n",
    "- The dataset has 440 rows and 8 columns - no missing values which is great!\n",
    "- Frozen, Grocery, Milk and Detergents_Paper have high correlations with each other\n",
    "- Looking at the histograms, most columns are right-skewed (most values are low, few are very high)\n",
    "- The box plots show lots of outliers - some customers spend WAY more than others\n",
    "- Channel and Region are categorical (1 or 2), the rest are numerical spending amounts\n",
    "- The value ranges are very different - Fresh goes up to 100k+ while Delicassen is much smaller\n",
    "- This looks like it might follow the Pareto principle - a few customers probably account for most sales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the data\n",
    "\n",
    "# make a copy so I don't mess up the original\n",
    "clean_customers = customers.copy()\n",
    "\n",
    "# drop Channel and Region columns - I don't need them for clustering\n",
    "if 'Channel' in clean_customers.columns:\n",
    "    clean_customers = clean_customers.drop('Channel', axis=1)\n",
    "\n",
    "if 'Region' in clean_customers.columns:\n",
    "    clean_customers = clean_customers.drop('Region', axis=1)\n",
    "\n",
    "# remove outliers using the IQR method\n",
    "# Q1 is the 25th percentile, Q3 is the 75th percentile\n",
    "Q1 = clean_customers.quantile(0.25)\n",
    "Q3 = clean_customers.quantile(0.75)\n",
    "IQR = Q3 - Q1  # this is the interquartile range\n",
    "\n",
    "# anything below Q1-1.5*IQR or above Q3+1.5*IQR is considered an outlier\n",
    "# the ~ means \"not\" - so I'm keeping rows that are NOT outliers\n",
    "clean_customers = clean_customers[~((clean_customers < (Q1 - 1.5 * IQR)) | \n",
    "                                     (clean_customers > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# see how many rows I removed\n",
    "print(f\"Started with: {customers.shape[0]} rows\")\n",
    "print(f\"Now have: {clean_customers.shape[0]} rows\")\n",
    "print(f\"Removed: {customers.shape[0] - clean_customers.shape[0]} rows\")\n",
    "\n",
    "clean_customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My cleaning decisions:**\n",
    "\n",
    "- I removed Channel and Region because they're categorical and I want to focus on spending patterns\n",
    "- I removed outliers using the IQR method to avoid extreme values messing up my clustering\n",
    "- After cleaning, I went from 440 rows to around 300-350 rows (depending on how strict the IQR filter is)\n",
    "- The data looks much cleaner now and should cluster better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to scale the data so all features are on the same scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit and transform the data in one step\n",
    "# this converts each column to have mean=0 and std=1\n",
    "customers_scale = scaler.fit_transform(clean_customers)\n",
    "\n",
    "# convert back to a dataframe to make it easier to work with\n",
    "customers_scale = pd.DataFrame(customers_scale, \n",
    "                                columns=clean_customers.columns,\n",
    "                                index=clean_customers.index)\n",
    "\n",
    "# check that it worked\n",
    "print(\"Scaled data:\")\n",
    "print(customers_scale.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# the mean should be close to 0 now\n",
    "print(\"Mean (should be ~0):\")\n",
    "print(customers_scale.mean())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# the std should be close to 1 now\n",
    "print(\"Std (should be ~1):\")\n",
    "print(customers_scale.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# first I need to find the best number of clusters\n",
    "# using the elbow method\n",
    "\n",
    "# try different numbers of clusters and see which is best\n",
    "inertias = []  # this will store how tight the clusters are\n",
    "\n",
    "for k in range(1, 11):\n",
    "    # create a model with k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    # fit it to my data\n",
    "    kmeans.fit(customers_scale)\n",
    "    # save the inertia (lower is better)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# plot the elbow curve\n",
    "# I'm looking for where the curve \"bends\" - that's the best k\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), inertias, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method - Finding Best Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking to the elbow we can choose 2 like the correct number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the elbow curve, I'll use 2 clusters\n",
    "kmeans_2 = KMeans(n_clusters=2, random_state=42)\n",
    "\n",
    "# fit the model to my scaled data\n",
    "kmeans_2.fit(customers_scale)\n",
    "\n",
    "# get the cluster labels (which cluster each customer belongs to)\n",
    "labels = kmeans_2.predict(customers_scale)\n",
    "\n",
    "# save the labels as a list\n",
    "clusters = kmeans_2.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the cluster labels to my cleaned data\n",
    "clean_customers['Label'] = clusters\n",
    "\n",
    "# now I can see which cluster each customer is in\n",
    "clean_customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many customers are in each cluster\n",
    "print(\"Customers per cluster:\")\n",
    "print(clean_customers['Label'].value_counts())\n",
    "\n",
    "# visualize it\n",
    "clean_customers['Label'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('How Many Customers in Each Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN \n",
    "\n",
    "# DBSCAN is different from K-Means\n",
    "# it can find clusters of any shape and marks outliers as -1\n",
    "\n",
    "# create the model\n",
    "# eps is how close points need to be to be in the same cluster\n",
    "dbscan = DBSCAN(eps=0.5)\n",
    "\n",
    "# fit it to my scaled data\n",
    "dbscan.fit(customers_scale)\n",
    "\n",
    "# get the labels (-1 means outlier)\n",
    "labels_dbscan = dbscan.labels_\n",
    "\n",
    "# add these labels to my data\n",
    "clean_customers['labels_DBSCAN'] = labels_dbscan\n",
    "\n",
    "# now I have both K-Means and DBSCAN labels\n",
    "clean_customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels_DBSCAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many in each DBSCAN cluster\n",
    "print(\"DBSCAN clusters:\")\n",
    "print(clean_customers['labels_DBSCAN'].value_counts())\n",
    "\n",
    "# count how many clusters (not including outliers)\n",
    "n_clusters = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
    "print(f\"\\nNumber of clusters: {n_clusters}\")\n",
    "\n",
    "# count outliers\n",
    "n_outliers = list(labels_dbscan).count(-1)\n",
    "print(f\"Number of outliers: {n_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to make plots\n",
    "def plot(x, y, hue):\n",
    "    sns.scatterplot(x=x, y=y, hue=hue)\n",
    "    plt.title('Detergents Paper vs Milk ')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare K-Means vs DBSCAN side by side\n",
    "# for Detergents_Paper vs Milk\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# K-Means on the left\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(data=clean_customers, \n",
    "                x='Detergents_Paper', \n",
    "                y='Milk', \n",
    "                hue='Label', \n",
    "                palette='viridis')\n",
    "plt.title('K-Means: Detergents_Paper vs Milk')\n",
    "\n",
    "# DBSCAN on the right\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(data=clean_customers, \n",
    "                x='Detergents_Paper', \n",
    "                y='Milk', \n",
    "                hue='labels_DBSCAN', \n",
    "                palette='viridis')\n",
    "plt.title('DBSCAN: Detergents_Paper vs Milk')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare K-Means vs DBSCAN for Grocery vs Fresh\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# K-Means\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(data=clean_customers, \n",
    "                x='Grocery', \n",
    "                y='Fresh', \n",
    "                hue='Label', \n",
    "                palette='viridis')\n",
    "plt.title('K-Means: Grocery vs Fresh')\n",
    "\n",
    "# DBSCAN\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(data=clean_customers, \n",
    "                x='Grocery', \n",
    "                y='Fresh', \n",
    "                hue='labels_DBSCAN', \n",
    "                palette='viridis')\n",
    "plt.title('DBSCAN: Grocery vs Fresh')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare K-Means vs DBSCAN for Frozen vs Delicassen\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# K-Means\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(data=clean_customers, \n",
    "                x='Frozen', \n",
    "                y='Delicassen', \n",
    "                hue='Label', \n",
    "                palette='viridis')\n",
    "plt.title('K-Means: Frozen vs Delicassen')\n",
    "\n",
    "# DBSCAN\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(data=clean_customers, \n",
    "                x='Frozen', \n",
    "                y='Delicassen', \n",
    "                hue='labels_DBSCAN', \n",
    "                palette='viridis')\n",
    "plt.title('DBSCAN: Frozen vs Delicassen')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what's different between the clusters\n",
    "# by looking at the average values\n",
    "\n",
    "print(\"K-Means cluster averages:\")\n",
    "print(\"=\"*60)\n",
    "kmeans_means = clean_customers.groupby('Label').mean()\n",
    "print(kmeans_means)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"DBSCAN cluster averages:\")\n",
    "print(\"=\"*60)\n",
    "dbscan_means = clean_customers.groupby('labels_DBSCAN').mean()\n",
    "print(dbscan_means)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# this helps me understand what makes each cluster unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm appears to perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which algorithm works better?**\n",
    "\n",
    "- K-Means seems to create clearer, more balanced clusters\n",
    "- DBSCAN found some outliers (labeled as -1) which is interesting - these might be unusual customers\n",
    "- Looking at the scatter plots, K-Means separates the data into distinct groups\n",
    "- The groupby means show that K-Means clusters have very different average spending patterns\n",
    "- For this dataset, I think K-Means with 2 clusters makes the most business sense - maybe retail vs food service customers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let me try different numbers of clusters to see which looks best\n",
    "\n",
    "cluster_numbers = [3, 4, 5]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "for i, n_clusters in enumerate(cluster_numbers):\n",
    "    # create and fit K-Means with n_clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(customers_scale)\n",
    "    labels_temp = kmeans.labels_\n",
    "    \n",
    "    # plot it\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    scatter = plt.scatter(clean_customers['Grocery'], \n",
    "                          clean_customers['Fresh'],\n",
    "                          c=labels_temp, \n",
    "                          cmap='viridis',\n",
    "                          alpha=0.6)\n",
    "    \n",
    "    plt.xlabel('Grocery')\n",
    "    plt.ylabel('Fresh')\n",
    "    plt.title(f'K-Means with {n_clusters} Clusters')\n",
    "    plt.colorbar(scatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# which one looks like it separates the data best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My thoughts on different cluster numbers:**\n",
    "\n",
    "- With 3 clusters: the separation is okay but one cluster seems too small\n",
    "- With 4 clusters: starting to look over-complicated, some clusters are very similar\n",
    "- With 5 clusters: too many! Hard to see clear differences between groups\n",
    "- I think 2 or 3 clusters work best for this data - keeps it simple and interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let me try different DBSCAN settings\n",
    "# eps = how close points need to be\n",
    "# min_samples = minimum points to form a cluster\n",
    "\n",
    "eps_values = [0.3, 0.5, 0.7]\n",
    "min_samples_values = [3, 5, 10]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "\n",
    "for i, eps in enumerate(eps_values):\n",
    "    for j, min_samples in enumerate(min_samples_values):\n",
    "        # create and fit DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan.fit(customers_scale)\n",
    "        labels_temp = dbscan.labels_\n",
    "        \n",
    "        # count clusters and outliers\n",
    "        n_clusters = len(set(labels_temp)) - (1 if -1 in labels_temp else 0)\n",
    "        n_outliers = list(labels_temp).count(-1)\n",
    "        \n",
    "        # plot it\n",
    "        plt.subplot(3, 3, i*3 + j + 1)\n",
    "        scatter = plt.scatter(clean_customers['Grocery'], \n",
    "                              clean_customers['Fresh'],\n",
    "                              c=labels_temp, \n",
    "                              cmap='viridis',\n",
    "                              alpha=0.6)\n",
    "        \n",
    "        plt.xlabel('Grocery')\n",
    "        plt.ylabel('Fresh')\n",
    "        plt.title(f'eps={eps}, min_samples={min_samples}\\n'\n",
    "                  f'Clusters: {n_clusters}, Outliers: {n_outliers}')\n",
    "        plt.colorbar(scatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# smaller eps = more clusters\n",
    "# larger min_samples = more outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What I learned about DBSCAN parameters:**\n",
    "\n",
    "- Smaller eps (like 0.3) creates more clusters and more outliers - very strict\n",
    "- Larger eps (like 0.7) creates fewer clusters and fewer outliers - more lenient\n",
    "- Higher min_samples makes it harder to form clusters, so more points become outliers\n",
    "- For this data, eps=0.5 and min_samples=5 seems like a good balance\n",
    "- DBSCAN is cool because it finds outliers automatically, but K-Means is easier to interpret for business use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}